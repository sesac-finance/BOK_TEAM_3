{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "!pip install openpyxl\n",
    "import openpyxl\n",
    "wb = openpyxl.Workbook()\n",
    "sheet = wb.active\n",
    "sheet.append([\"날짜\",\"제목\",\"본문\"])\n",
    "for i in range(1,101): # 마지막 페이지 입력하기 101\n",
    "    URL = 'https://finance.naver.com/research/debenture_list.nhn?keyword=&brokerCode=&searchType=writeDate&writeFromDate=2011-01-01&writeToDate=2021-12-31&x=0&y=0&page='+str(i)\n",
    "    response = requests.get(URL)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    list_minutes = soup.select('div[id=\"contentarea_left\"] div[class=\"box_type_m\"] tr')\n",
    "    for minutes in list_minutes:\n",
    "        try:\n",
    "            title=minutes.select('a')[0].text\n",
    "            sub_link = minutes.select('a')[1].attrs['href']\n",
    "            date = datetime.strptime('20'+ minutes.select('.date')[0].text, \"%Y.%m.%d\")\n",
    "            if str(sub_link).endswith('.pdf'):\n",
    "                print(date.strftime('%Y.%m.%d'))\n",
    "                print(title)\n",
    "                print(sub_link)\n",
    "                file_res = requests.get(sub_link)\n",
    "                sheet.append([ds.strftime('%Y.%m.%d'),title,sub_link])\n",
    "            else:\n",
    "                pass\n",
    "        except:\n",
    "            pass\n",
    "wb.save(\"bond_report.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 2 - x download pdf/ getting urls only\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import csv\n",
    "f = open('bond_report.csv','w',newline='')\n",
    "wr = csv.writer(f)\n",
    "for i in range(1,102): # 마지막 페이지+1 입력\n",
    "    URL = 'https://finance.naver.com/research/debenture_list.nhn?keyword=&brokerCode=&searchType=writeDate&writeFromDate=2011-01-01&writeToDate=2021-12-31&x=0&y=0&page='+str(i)\n",
    "    response = requests.get(URL)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    list_minutes = soup.select('div[id=\"contentarea_left\"] div[class=\"box_type_m\"] tr')\n",
    "    for minutes in list_minutes:\n",
    "        try:\n",
    "            title=minutes.select('a')[0].text\n",
    "            sub_link = minutes.select('a')[1].attrs['href']\n",
    "            date = datetime.strptime('20'+ minutes.select('.date')[0].text, \"%Y.%m.%d\")\n",
    "            date_convert = date.strftime('%Y.%m.%d')\n",
    "            print(date_convert)\n",
    "            print(title)\n",
    "            print(sub_link)\n",
    "            wr.writerow([date_convert,title,sub_link])\n",
    "        except:\n",
    "            pass\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#링크안 pdf text 긁어오기\n",
    "import os\n",
    "from tika import parser\n",
    "import pandas as pd\n",
    "import csv\n",
    "bond_report = pd.read_csv(\"bond_report.csv\", header=0)\n",
    "bond_report.columns = ['date','title','url']\n",
    "content = []\n",
    "for url in bond_report['url']:\n",
    "    try:\n",
    "        content.append(parser.from_file(url)['content'].replace('\\n', ''))\n",
    "    except:\n",
    "        content.append(\"NA\")\n",
    "        pass\n",
    "bond_report['content'] = content\n",
    "bond_report \n",
    "bond_report.to_csv(r'/Users/soojinkim/kdigital/mini_project_1/bond_report_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
